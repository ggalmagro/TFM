\documentclass[review]{elsarticle}

\usepackage{graphics, float, url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{lineno,hyperref}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{chngpage}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{float}
\usepackage[ruled, lined, onelanguage, linesnumbered]{algorithm2e}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\newcommand{\myfloatalign}{\centering}
\modulolinenumbers[5]

\usepackage{epstopdf}
\epstopdfDeclareGraphicsRule{.tiff}{png}{.png}{convert #1 \OutputFile}
\AppendGraphicsExtensions{.tiff}

\journal{Applied Soft Computing}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{definition}{Definition}
\begin{document}

\begin{frontmatter}

\title{Enhancing instance-level constrained clustering through differential evolution}

\author[mymainaddress]{Germ\'an Gonz\'alez Almagro\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{germangalmagro@ugr.es}

\author[mymainaddress]{Salvador Garc\'ia}

\author[mymainaddress]{Another Juan}

\address[mymainaddress]{Department of Computer Science and Artificial Intelligence, University of Granada, 18071 Granada, Spain}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\textcolor{red}{Revisar que todos los subindices en los que aparecan comas lleven corchetes. Revisar que las asignaciones en los algoritmo sean flechas y no =. Revisar que restrictions nunca aparece, siempre constraints. Revisar que es siempre instance-level y no instance level. Comprobar que es siempre dataset y no data set. Puedo decir en alguna parte que se hace una iterpretación relajada de las restricciones. Tengo que decidir como llamar a la funcion de coste, "cost", "fitness" o "heuristic". Cambiar los = de las hipotesis por :. Cambiar constraints set por constraint set}

\section{Introduction}

%%CORREGIR ESTA SECCION

One of the most widely known and studied data analysis tools is clustering. It is one of the most successful methods that has been able to provide solutions in a large number of knowledge fields, such as medicine, marketing, banking or psychology, among others.

Clustering is the task of grouping instances of a dataset into subsets, so that new information can be extracted from them. The criterion used to assign an instance to a given cluster is the similarity to the rest of elements in that cluster, and the dissimilarity to the rest of instances of the dataset. In this way, the clustering process obtains subsets of instances of the initial dataset that present high intra-similarity and low inter-similarity.

It is well known that the clustering problem is $\mathbf{NP}$-hard, therefore finding the optimal solution is not feasible with the technology we have today. However, there are many approximated resolution methods, including constrained clustering.

Constrained clustering is a semi-supervised learning method. Its goal is to find a partition of the dataset that meets the proper characteristics of a clustering method result, in addition to satisfying a certain constraints set. In \cite{davidson2007survey} we find two constraint models, as well as an extensive review of some resolution methods. This study focuses specifically on instance-based constraints, which will be defined in Section \ref{sec:background}.

[Some of the constrained clustering applications mentioned in \cite{davidson2007survey} include: object recognition and tracking in video data, grouping of face images according to their orientation, detection of simultaneous appearance of genes and proteins in biological data, grouping of text documents according to their content, grouping of URLs obtained as a result of ambiguous searches according to their content, identification of speakers in audio data, and the widely known grouping of vehicles and lanes based on GPS data.]

It should be noted that, as we will study in more depth in Section \ref{sec:feas}, the incorporation of constraints to clustering modifies its complexity, making it harder. The SHADE algorithm has proven to be one of the most successful methods to solve real-domain problems in an approximate way. The aim of this paper is to propose an adaptation of it to solve constrained clustering. 

There have been previous attempts to adapt population-based methods to constrained clustering, in particular in \cite{de2017comparison} an adaptation of the BRKGA algorithm is proposed. There are also some adaptations of swarm intelligence based methods, such as the one presented in \cite{xu2013improving}. As far as this paper is concerned, we will use some of the techniques presented in \cite{de2017comparison} to adapt SHADE to constrained clustering, as well as we propose a new cost function to improve the results. We will properly compare the results obtained by both methods. On the other hand we will check whether the population-based methods represent an improvement over some of the non heuristic-based methods, presented in \cite{davidson2007survey} and briefly discussed in this paper in Section \ref{sec:NHres}.

Regarding the organization of this paper, Section \ref{sec:background} reviews the general knowledge concerning constrained clustering. In the Section \ref{sec:brkga} we will briefly review the BRKGA algorithm and its adaptation for constrained clustering. On the other hand the SHADE algorithm will be presented in the section \ref{sec:SHADE}, as well as in it we will present its adaptation for constrained clustering and a new cost function. Sections \ref{sec:expSetup} to \ref{sec:analisis} present the experimental setup, results and their analysis. Finally, in Section \ref{sec:conclusiones} we discuss conclusions and thoughts about future work.

\section{Background} \label{sec:background}

%\subsection{Constrained Clustering}

In most clustering applications it is common to have some kind of information about the dataset to be analyzed. In constrained clustering this information is given in the form of pairs of instances. A constraint states whether the instances which it refers to must, or must not, be assigned to the same cluster. It is possible to obtain a better result by using this type of information than by using completely unsupervised clustering algorithms

Before formalizing the definition of constraints we must give a notation for the dataset. We will note our dataset as $X$, which is a set of $N$ instances with $D$ features. We well refer to the instances of $X$ as $x$. Now, given the definition of dataset, we can distinguish between two types of constraints:

\begin{itemize}

	\item Must-link constraints $c_=(x_j,x_i)$: instances $x_i$ and $x_j$ from $X$ must be placed in the same cluster.

	\item Cannot-link constraints $c_{\neq}(x_i,x_j)$: instances $x_i$ and $x_j$ from $X$ cannot be assigned to the same cluster.

\end{itemize}

The goal of constrained clustering is to find a partition (or clustering) of $K$ clusters $C = {c_1 \cdots c_K}$ of the dataset $X$ that ideally satisfies all constraints in the constraints set. As in the original clustering problem, it must be fulfilled that the sum of instances in each cluster $c_i$ is equal to the number of instances in $X$, which we have defined as $N = |X| = \sum_{i = 1}^{K} |c_i|$.

Knowing how a constraint is defined, Must-link constraints---from now on ML---are an example of an equivalence relation; therefore, ML constraints are reflexive, transitive and symmetric. This way, given constraints $c_=(x_a,x_b)$ and $c_=(x_b,x_c)$ then $c_=(x_a,x_c)$ is verified. In addition, if $x_a \in c_i$ and $x_b \in c_j$ are related by $c_=(x_a,x_b)$, then $c_=(x_c,x_d)$ is verified for any $x_c \in c_i$ and $x_d \in c_j$ \cite{xu2013improving}\cite{davidson2007survey}.

On the other hand, Cannot-link constraints---from now on CL---do not constitute an equivalence relation. However, analogously, given $x_a \in c_i$ and $x_b \in c_j$, and the constraint $c_{\neq}(x_a,x_b)$, then it is also true that $c_{\neq}(x_c,x_d)$ for any $x_c \in c_i$ and $x_d \in c_j$ \cite{davidson2007survey}.

\subsection{The Feasibility Problem} \label{sec:feas}

Given that constrained clustering adds a new element to the clustering problem, we must consider how this element affects the problem's complexity. The feasibility problem for non-hierarchical constrained clustering was defined by \cite{davidson2005clustering} as in Definition \ref{def1}.

\begin{definition}
	
	\textbf{Feasibility Problem}: given a dataset $X$, a set of constraints $R$, and the bounds on the number of clusters $K_l \leq k \leq K_u$, does there exist a partition $C$ of $X$ with $k$ clusters such that all constraints in $R$ are satisfied? \cite{davidson2007survey}\cite{davidson2005clustering}
	\label{def1}
	
\end{definition}

In \cite{davidson2005clustering} it is proven that, when $K_l = 1$ and $K_u \ge 3$, the feasibility problem for constrained clustering is $\mathbf{NP}$-complete, by reducing it from the Graph K-Colorability problem (it is also proven that it is not harder, so both have the same complexity). Table \ref{tab:feasibility} shows the feasibility problem's complexity for different types of constraints.

\begin{table}[!h]
	\centering
	%\setlength{\arrayrulewidth}{1mm}
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.2}
	%\resizebox{\textwidth}{!}{
		\begin{tabular}{c c}
			\hline
			Constraints & Complexity \\
			\hline
			Must-Link & $\mathbf{P}$\\
			Cannot-Link & $\mathbf{NP}$-complete\\
			ML and CL & $\mathbf{NP}$-complete\\
			\hline
			
		\end{tabular}%}
	\caption{Feasibility problem complexity \cite{davidson2005clustering}}
	\label{tab:feasibility}
\end{table}

These complexity results show that the feasibility problem with CL constraints is intractable and hence constrained clustering is intractable too. For more details on the complexity of constrained clustering see \cite{davidson2005clustering}.

\textcolor{red}{El parrafo de abajo esta bien?}

Intractable problems are hard to solve with deterministic and exact methods. That is the reason why heuristics-guided and population-based algorithms constitute good approaches to find quality solutions to the constrained clustering problem.

\subsection{Short Introduction to Differential Evolution}

\textcolor{red}{Eliminar toda la notacion matematica de esta seccion?}

To introduce Differential Evolution---from now on DE---we take as a starting point the review of the state-of-the-art carried out by \cite{das2011differential}. A highly detailed analysis of DE and its variants can also be found in \cite{das2011differential}.

The first article written on DE emerged in 1995 as a technical report by \cite{noman2008differential}. From that point on, DE's reputation was consolidated in conferences and competitions where it was able to obtain competitive results that rivaled those achieved by the state-of-the-art. In particular, DE excelled in real-coded optimization problems.

DE arose as a variant of population-based evolutionary algorithms, its goal being to perform an intelligent search in the problem's solution space in the most optimized and effective way possible.

Like most evolutionary algorithms, DE uses a population of individuals $P$, where each individual $p_i$ is a vector of real values $p_i = \{p_{i,1},\cdots,p_{i,D}\}$ that serve as parameters for the function to be optimized. These individuals are considered solutions to the problem.

To guide the optimization process, we use an objective function. DE's task is to seek the parameter vector $p_i^*$ that minimizes a function such that $f(p_i)(f: \Omega \subseteq \mathfrak{R}^D \rightarrow \mathfrak{R})$, where $f(p_i^*) < f(p_i)$ for all $p_i \in \Omega$ with $\Omega$ a nonempty finite set that is the search domain.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.3]{Figures/DEop.png}
	\caption{DE's optimization process \cite{das2011differential}.}\label{img:DE}
\end{figure}

DE's optimization process works through a simple cycle of stages, presented in Figure \ref{img:DE}, that we can summarize in as follows:

\begin{itemize}
	
	\item \textbf{Initialization of Vectors}: DE begins with a randomly initialized population of $N$ $D$-dimensional arrays of parameters. These vectors constitute candidate solutions for the problem to solve.
	
	\item \textbf{Difference-vector-based mutation}: DE introduces diversity into the population via mutation operator. This operator is applied to each individual of the population, known in this context as the parent vector. It combines the parent vector with two randomly selected vectors from the population by applying an arithmetic operator to them. The result of this process is a mutant vector.
	
	\item \textbf{Crossover/Recombination}: The mutant vector exchanges some components with its associated parent to form a trial vector, which will be the candidate to replace the parent vector. Several strategies exist to generate this trial vector, which we will not delve into; however, they can be found in \cite{das2011differential}.
	
	\item \textbf{Selection}: The newly generated trial vector is compared with its associated parent using the objective function. If the trial vector is better than the parent, then it will replace the parent in the next generation. It is worth noting that, following this strategy, the population never degenerates but only gets better or remains the same.
	
\end{itemize}

Even though DE was a major breakthrough in terms of real optimization, many variants have been developed since its invention that provide better results. We will focus on the SHADE variant, which adaptively optimizes some of DE's parameters (see Section \ref{sec:SHADE}).

\subsection{(Classic) Resolution Methods} \label{sec:NHres}

\textcolor{red}{Clasicos clasicos no son, puedo cambiar classic por non-heuristic?}

Constrained clustering has many applications and has been widely studied in the literature. The first adaptation of a classic clustering method for constrained clustering was proposed by \cite{wagstaff2001constrained}. It involved modifying the widely studied K-means algorithm to take into account instance-level constraints: the already known ML and CL. This method was named COP-kmeans.

The authors of \cite{antoine2012cecm} propose an adaptation of the evidential C-means algorithm, within the fuzzy clustering framework, for constrained clustering. The particularity of this resolution method is that the membership of instances to a cluster is defined by a probabilistic belief function. This method redefines constraints from the point of view of belief functions and includes them in the cost function.

A modification of CVQE (Constrained Vector Quantization Error), LCVQE, is proposed in \cite{pelleg2007k}. The adaptation of CVQE for constrained clustering was proven to produce high quality results, at the cost of a very high computational complexity. LCVQE (Linear CVQE) introduces a modification of CVQE's objective function to make it more intuitive and less computationally complex. The experimentation resulted in a dramatic improvement of clustering quality over both noisy and clean constraints sets.

TVClust (Two Views Clustering) and RDP-means (Relation Dirichlet Process - Means) were proposed by \cite{khashabi2015clustering}. TVClust is able to incorporate the constraints into the clustering problem by making a relaxed interpretation of them. The authors model the dataset and constraints in different ways, perform clustering methods on them and try to find a consensus between both interpretations. Using this model as a basis, the authors derive the deterministic algorithm RDP-means. This method can be viewed as an extension of K-means that includes side information (constraints) and has the property that the number of clusters ($K$) does not need to be specified.

\textcolor{red}{Decir que voy a comparar con ellos? Incluir mas métodos ``clasicos''?}

\section{Brief Review of the BRKGA Algorithm} \label{sec:brkga}

The biased random-key genetic algorithm---from now on BRKGA---was first proposed by \cite{gonccalves2011biased} as a generalization of the random-key genetic algorithm \cite{bean1994genetic}.

In BRKGA, each solution---also called individual---is represented as a vector of values within the interval $[0,1]$. However, this vector is not a solution to the problem but a representation of it. This allows us to abstract the details of the problem in order to apply a genetic algorithm that operates with real coding. Therefore, a deterministic decoder is necessary to obtain the actual solution to the problem from the vector that encodes it.

BRKGA also requires a fitness function to evaluate each individual. This function takes the decoded solution as input and provides its fitness value as output. It is designed specifically for each problem.

The BRKGA optimization process is completely independent of the problem to solve. First, a population $P$ of $p_{size}$ vectors of random-keys is initialized, and this will be the initial generation. Each random-key vector $p_i$ has $N$ random-keys in it. The population is sorted by the fitness value $f_i$ of each $p_i$ to select the first $p_e$ individuals, this is, the elite of the population. The elite will be preserved in the next generation without modification. To introduce diversity, a number $p_m$ of new random-keys vectors are also included in the next generation. The remaining individuals of the new generation ($p_{size} - p_e - p_m$) are obtained through crossovers between elite and non-elite parents \cite{de2017comparison}.

\subsection{Adaptation of BRKGA for Constrained Clustering} \label{sec:AdaptationofBRKGA}

An adaptation of BRKGA for constrained clustering was proposed in \cite{de2017comparison}.

Their method randomly initializes a population of random-key vectors, each one with $N$ random-keys. The decoder divides the interval $[0,1]$ in $K$ intervals, so there exists a correspondence between each random-key and the integer corresponding to the interval which it lies in. Table \ref{tab:decodingrk} shows an example of random-key decoding for $K = 3$. Note that extreme values 0 and 1 can also appear in a random-key vector.

\begin{table}[!h]
	\centering
	%\setlength{\arrayrulewidth}{1mm}
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.2}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		Index & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
		\hline
		Random-key & 0.12 & 0.37 & 0.66 & 0.56 & 0.00 & 0.97 & 0.23 & 0.25 & 0.15 & 1.00 \\
		\hline
		Clusters & 1 & 2 & 2 & 2 & 1 & 3 & 1 & 3 & 1 & 3 \\
		\hline

	\end{tabular}}
	\caption{Random-key decodification example \cite{de2017comparison}}
	\label{tab:decodingrk}
\end{table}

Once the decoded solution has been obtained, the fitness value of the solution is computed as in Equation \eqref{eq1}.

\begin{equation}
f_i = z_i + \overbrace{(\mu * N * {infeasibility}_i)}^{penalty}
\label{eq1}
\end{equation}

Where $\mu$ is a high value, $infeasibility_i$ is the number of non-satisfied constraints and $z_i$ is the within-cluster-sum-of-squares which can be computed as in Equation \eqref{eq2}.

\begin{equation}
z_i = \sum_{c_j \in C_i} \left[ \frac{\sum_{x_a, x_b \in c_j} d_D^2(x_a,x_b)}{|c_j|}\right]
\label{eq2}
\end{equation}

Where $C_i$ is the partition of dataset $X$ defined by solution $p_i$, and $(x_a, x_b)$ are instances in the cluster $c_j$ such that $a \neq b$ and the Euclidean distance ($d_D^2(\cdot, \cdot)$) between each pair of instances in $c_i$ is included in the sum only once.

In \cite{de2017comparison}, the authors also added a new element to BRKGA, a local search procedure. This local optimization method is applied to each decoded individual in each generation, but its results are not transferred to the individual, so that diversity is maintained. Thus, the individuals produced by the local search are only used to update the best solution found so far if needed. For more details on BRKGA see \cite{de2017comparison}.

\section{The SHADE Optimization Method} \label{sec:SHADE}

Success history based adaptive differential evolution---from now on SHADE---is an optimization method based on differential evolution. Proposed by \cite{tanabe2013success}, it was an improvement for the JADE model, created by \cite{zhang2009jade}.

Unlike JADE, which only considers the parameters used in the previous generation, SHADE uses a historical record of successful parameters as a mechanism for adapting parameters involved in the process of creating new generations.

\subsection{SHADE Operators}

As in differential evolution, SHADE employs mutant generation, crossover and replacement operators to explore the space of solutions and bring in new generations of individuals.

The mutation strategy used by SHADE is known as current-to-$p$best/1, and the expression that generates new individuals is described in Equation \eqref{eq3}.

\begin{equation}
m_{[i,G]} = p_{[i,G]} + F_i * (p_{[pbest, G]} - p_{[i,G]}) + F_i * (p_{[r1, G]} - p_{[r2,G]})
\label{eq3}
\end{equation}

Where $m_{[i,G]}$ is the mutant vector, which is generated with an individual $p_{[i,G]}$ from the population [serving as a starting point]. Indices $r1$ and $r2$ are random values in the range $[0,N]$, different from $i$ and from each other. The individual $p_{[pbest, G]}$ is randomly selected from among the best $N \times pbest\;|\;pbest\in [0,1]$ individuals in the population. This way, the parameter $p$ controls the greediness of the mutation strategy. The parameter $F_i$ defines the magnitude of the operator.

After generating the mutant vector $m_{[i,G]}$, it is combined with the parent $p_{[i,G]}$ vector by means of a crossover operator; the result is the trial vector. SHADE uses the Binomial crossover operator, which is defined by the Equation \eqref{eq4}.

\begin{equation}
t_{[j,i,G]} = \left\{ \begin{array}{lc}
m_{[j,i,G]} &   if \;\; \text{rand}[0,1) \le CR_i \;\; or \;\;j = j_{rand} \\
p_{[j,i,G]} &  \text{otherwise}
\end{array}
\right.
\label{eq4}
\end{equation}

Where $rand[0,1)$ is a random number selected from a normal distribution in the range $[0,1)$, $j_{rand}$ is a random integer selected from the range $[1,D]$, and $CR \in [0,1]$ is the crossover ratio.

The SHADE replacement operator determines the survivors individuals for the next generation. It compares each parent $x_{[i,G]}$ with the trial vector $_{[j,i,G]}$, generated based on it. The best one of these two individuals will survive for the next generation. Equation \eqref{eq5} shows this concept.

\begin{equation}
p_{[i,G + 1]} = \left\{ \begin{array}{lc}
t_{[i,G]} &   if \;\; f(t_{[i,G]}) \le f(p_{[i,G]}) \\
p_{[i,G]} &  \text{otherwise}
\end{array}
\right.
\label{eq5}
\end{equation}

To maintain diversity, SHADE can make use of an external archive $A$, in which parents $p_{[i,G]}$ who were replaced by their associated trial vector $t_{[i,G]}$ are saved. To make use of the archive, we will consider that the individual $p_{[r2,G]}$ from Equation \eqref{eq3} is selected from $P \cup A$. The archive size is the same as the population size. When the size of $A$ exceeds the size of $P$, random individuals are selected for removal to make room for new ones.

The parameters $p$, $F_i$ and $CR_i$ are difficult to adjust and their selection is not trivial, as they largely determine the success of the optimization process. These are the parameters that SHADE adaptively optimizes using the mentioned history record.

\subsection{SHADE Parameter Adaptation Method}

The SHADE method stores in memory a structure with $H$ entries for parameters $F_i$ and $CR_i$, as shown in Table \ref{tab:SHADEmemory}. This structure is initialized with the value $0.5$ for all entries.

\begin{table}[!h]
	\centering
	%\setlength{\arrayrulewidth}{1mm}
	\setlength{\tabcolsep}{13pt}
	%\renewcommand{\arraystretch}{0.9}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 Index & 1 & 2 & $\cdots$ & $H - 1$ & $H$ \\
		 \hline
		 \hline
		 $M_{CR}$ & $M_{[CR,1]}$ & $M_{[CR,1]}$ & $\cdots$ & $M_{[CR,H-1]}$ & $M_{[CR,H]}$ \\
		 \hline
		 $M_{F}$ & $M_{[F,1]}$ & $M_{[F,1]}$ & $\cdots$ & $M_{[F,H-1]}$ & $M_{[F,H]}$ \\
		\hline

	\end{tabular}}
	\caption{Historical memory $M_{CR}$, $M_{F}$ used by SHADE \cite{tanabe2013success}}
	\label{tab:SHADEmemory}
\end{table}

In each generation, parameters $CR_i$ and $F_i$ are calculated on the basis of the existing history by applying Equations \eqref{eq6} and \eqref{eq7}:

\begin{equation}
CR_i = \text{randn}_i(M_{[CR,r_i]}, 0.1)
\label{eq6}
\end{equation}

\begin{equation}
F_i = \text{randc}_i(M_{[F,r_i]}, 0.1)
\label{eq7}
\end{equation}

where $randn(\mu, \sigma^2)$ and $randc(\mu, \sigma^2)$ are random values from normal and Cauchy distributions respectively, with mean $\mu$ and variance $\sigma^2$. When a value for $CR_i$ outside of the range $[0,1]$ is generated, it is replaced by the corresponding limit value. If $F_i > 1$, then it is truncated to $1$; conversely, if $F_i < 0$, then Equation \eqref{eq7} is applied as many times as needed to obtain a legal value.

SHADE keeps two auxiliary sets $S_{CR}$ and $S_F$ to store the $CR_i$ and $F_i$ values that were successfully used to generate a trial vector that replaced the parent. At the end of each generation, the content of the memory $M$ is updated following Equations \eqref{eq8} and \eqref{eq9}.

\begin{equation}
M_{[CR,h,G+1]} = \left\{ \begin{array}{lc}
\text{mean}_{WA} (S_{CR}) &   if \;\; S_{CR} \neq \emptyset \\
M_{[CR,h,G+1]} &  otherwise
\end{array}
\right.
\label{eq8}
\end{equation}

\begin{equation}
M_{[F,h,G+1]} = \left\{ \begin{array}{lc}
\text{mean}_{WL} (S_{F}) &   if \;\; S_{F} \neq \emptyset \\
M_{[F,h,G+1]} &  otherwise
\end{array}
\right.
\label{eq9}
\end{equation}

The $h \;\; (0 \le h \le H)$ index specifies the position of the memory $M$ to be updated. This index is initialized to $0$ at the beginning of the optimization process and increased by one after every generation. When $h \ge H$ then $h$ is reset to 1. It is worth noting that, when there is a generation with no trial vectors successfully replacing their parents, no update of $M$ is done.

In Equation \ref{eq8}, the term $mean_{WA} (S_{CR})$ is the weighted mean, which is computed following Equations \eqref{eq10} and \eqref{eq11}, proposed by \cite{peng2009multi} to prevent $CR$ from converging to small values.

\begin{equation}
\text{mean}_{WA} (S_{CR}) = \sum_{i = 1}^{|S_{RC}|} \omega_i * S_{[RC,i]}
\label{eq10}
\end{equation}

\begin{equation}
\omega_i = \frac{\Delta f_i}{\sum_{i = 1}^{|S_{RC}|} \Delta f_i}
\label{eq11}
\end{equation}

where $\Delta f_i = |f(t_{[i,G]}) - f(p_{[i, G]})|$, which is the amount of improvement obtained from the trial vector with respect to the parent.

In Equation \ref{eq9} the term $mean_{WL} (S_{F})$ refers to the weighted Lehmer mean, which is computed as in Equation \eqref{eq12} (proposed by \cite{tanabe2013success}).

\begin{equation}
\text{mean}_{WL} (S_{F}) = \frac{\sum_{i = 1}^{|S_{F}|} \omega_i * S^2_{[F,i]}}{\sum_{i = 1}^{|S_{F}|} \omega_i * S_{[F,i]}}
\label{eq12}
\end{equation}

Unlike parameters $S_{CR}$ and $S_F$, the parameter $pbest$ from Equation \eqref{eq3}, used to set the greediness of the mutation strategy, is not included in the adaptive optimization process. However, it is not static: it is calculated for every individual $p_i$ of the population following Equation \eqref{eq13}.

\begin{equation}
pbest_i = \text{randn}[2/N, 0.2]
\label{eq13}
\end{equation}

such that there is always at least two individuals to choose between.

\subsection{Adaptation of SHADE for constrained clustering}

For the adaptation of SHADE to the problem of constrained clustering, we will take some of the elements that allowed us to adapt it to BRKGA. To begin with, we will use random-keys to create the individuals of the population; this way, each individual is defined as a vector of random-keys. Additionally, we will also reuse the same initialization method and the same decoder used in BRKGA (see Section \ref{sec:AdaptationofBRKGA}).

However, we propose a new fitness function to evaluate the quality of the individuals of the population, Equation \eqref{eq14}, whose element $z_i$ is obtained as in Equation \eqref{eq2}.

\begin{equation}
f_i = z_i * (infeasibility + 1)
\label{eq14}
\end{equation}

Note that in Equation \eqref{eq14} no parameter is involved that cannot be calculated from the dataset or constraints, whereas in Equation \eqref{eq1} the $\mu$ parameter must be calculated and optimized for each individual problem.

We found that with the fitness function defined in \eqref{eq1} there is no competition between the penalty term and the within-cluster-sum-of-squares term, because the penalty is always significantly larger than it or zero. This can bias the exploration of the solution space, restricting it in practice to those that satisfy all the constraints, even if the within-cluster-sum-of-squares is still improvable by moving two instances involved in a constraint to different clusters without violating that constraint.

With Equation \eqref{eq14} we try to find a trade-off between the within-cluster-sum-of-squares and the penalty term, allowing solutions that violate a certain number of constraints to compete with those who violate a smaller number of them but score a better within-cluster-sum-of-squares.

To make these concepts clear we consider a toy dataset and three partitions over it. The dataset from Figure \ref{img:toydatasets} contains 100 instances and a single ML constraint between two instances from the same class. Partitions $C_0$---displaying the true labels---and $C_1$ do not violate the constraint ($infeasibility = 0$), whereas $C_2$ does violate it ($infeasibility = 1$). However, $C_2$ shows a possibility, for later iterations, of adding both instances to their nearest cluster while also satisfying the ML constraint.

\begin{figure}[bth]
	\myfloatalign
	{\includegraphics[width=.45\linewidth]{Figures/Dataset}} \quad
	%\subfloat[Imagen original]
	{\includegraphics[width=.45\linewidth]{Figures/C0}} \quad
	%\subfloat[Clustering sin restricciones]
	{\includegraphics[width=.45\linewidth]{Figures/C1}} \quad
	%\subfloat[Clustering con restricciones]
	{\includegraphics[width=.45\linewidth]{Figures/C2}} \quad
	\caption{Three partitions over a toy dataset.}
	\label{img:toydatasets}
\end{figure}

Table \ref{tab:fitnessfunctions} shows fitness values for each partition from Figure \ref{img:toydatasets}. We refer to the function presented in Equation \eqref{eq1} as $f_1$ and to the function presented in Equation \eqref{eq14} as $f_2$.

\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.3}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c c c c c c}
			\hline
			\multirow{2}{*}{Partition} &
			\multicolumn{2}{c}{Expression} &&
			\multicolumn{2}{c}{Value} \\
			\cline{2-3} \cline{5-6}
			& $f_1$ & $f_2$ && $f_1$ & $f_2$ \\
			\hline
			$C_0$ & $z_0$ & $z_0$ && $0.457$  & $0.457$  \\
			$C_1$ & $z_1$ & $z_1$ && $0.540$  & $0.540$  \\
			$C_2$ & $z_2 + (\mu * N * infs)$ & $z_2 * (infs + 1)$  && $0.503 + 1000 = 1000.503$ &  $0.503 * 2 = 1.005$ \\
			\hline
			
		\end{tabular}}
		%\end{adjustwidth}
		
	\caption{Expression and value of fitness functions over three partitions. ($\mu = 10$)}
	\label{tab:fitnessfunctions}
\end{table}

Results in Table \ref{tab:fitnessfunctions} are a clear example of the above. In the general case, by using $f_2$ the penalty for violating constraints is proportional to the within-cluster-sum-of-squares---the lower it is, the lower the penalty. This is not the case with $f_1$, whose penalty term is independent of the within-cluster-sum-of-squares, which can result in a difference of several orders of magnitude between partitions satisfying different amounts of constraints.

As in \cite{de2017comparison} we apply a local optimization procedure to the individuals of the population, but without transferring its results to the original individual in order to maintain diversity. The difference is that we do not apply it to the whole population but only to a number $p_e$ of individuals considered as the elite of the population. The percentage of individuals considered as elite must be determined for each case.

Algorithm \ref{alg:SHADE} describes the overall SHADE optimization process and is an adaptation of the one that can be found on \cite{tanabe2013success} to include the local search procedure presented in Algorithm \ref{alg:LS}.

\begin{algorithm}
	\SetNlSty{textbf}{[}{]}
	\SetNlSkip{0.5em}
	\setstretch{1.2}
	\SetKwFunction{Sort}{Sort}
	\SetKwFunction{LocalSearch}{LocalSearch}
	\KwIn{Dataset $X$, constraints sets $c_=$ and $c_{\neq}$, population size $p_{size}$, elite size $p_e$, number of clusters $K$.}
	\tcp{Initialization phase}
	$G \leftarrow 0$\\
	Initialize population $P_0 \leftarrow \{p_{[1,0]}, \cdots, p_{[p_{size},0]}\}$\\
	Initialize all values in $M_{CR}$, $M_F$ to 0.5\\
	$A \leftarrow \emptyset$; $h \leftarrow 1$\\
	\tcp{Main loop}
	\While{Termination criteria are not met}{
		
		$S_{CR} \leftarrow S_F \leftarrow \emptyset$\\
		$P_G \leftarrow$ \Sort{$P_G$} \tcp{Sort the population}
		\tcp{Apply a LS procedure to the elite of the population}
		\LocalSearch{$\{p_{[1,G]}, \cdots, p_{[p_{e},G]}\}$}\\
		\For{$i \in [1,p_{size}]$}{
			$r_i \leftarrow$ randInt$[1,H]$\\
			$CR_{[i,G]} \leftarrow$ randn$_i(M_{[CR,r_i]}, 0.1)$\\
			$F_{[i,G]} \leftarrow$ randc$_i(M_{[F,r_i]}, 0.1)$\\
			$p_{[i,G]} \leftarrow$ rand$[N/2, 0.2]$\\
			Generate trial vector $t_{[i,G]}$ by current-to-$p$best/1/bin\\
			
			\eIf{$f(t_{[i,G]}) \le f(p_{[i,G]})$}{
				$p_{[i,G + 1]} \leftarrow t_{[i,G]}$
			}{
				$p_{[i,G + 1]} \leftarrow p_{[i,G]}$
			}
			
			\If{$f(t_{[i,G]}) < f(p_{[i,G]})$}{
				$p_{[i,G]} \rightarrow A$;
				$CR_{[i,G]} \rightarrow S_{CR}$;
				$F_{[i,G]} \rightarrow S_{F}$
			}
			
		}
		Whenever $|A| > |P|$, randomly select an individual from $A$ to be deleted so that $|A| \le |P|$\\
		\If{$S_{CR} \neq \emptyset$ \textbf{and} $S_{F} \neq \emptyset$}{
			Update $M_{[CR,h]}$ and $M_{[F,h]}$ based on $S_{CR}$ and $S_{F}$\\
			$h \leftarrow (h + 1) \mod H$
		}
		$G++$
	}
	\caption{Modified SHADE}\label{alg:SHADE}
\end{algorithm}

\begin{algorithm}
	\SetNlSty{textbf}{[}{]}
	\SetNlSkip{0.5em}
	\setstretch{1.2}
	\SetKwFunction{RandomShuffle}{RandomShuffle}
	\SetKwRepeat{Do}{do}{while}
	\KwIn{Dataset $X$, constraints sets $c_=$ and $c_{\neq}$, decoded random-key vector (solution) $S$, number of clusters $K$.}
	\BlankLine
	\While{$improvement$}{
		$improvement \leftarrow$ \texttt{false} \\

		$s_i \leftarrow $ Select random object from $S$\\
		\tcp{Random shuffle labels set}
		$RSL \leftarrow $ \RandomShuffle{$\{1,\cdots,K\}$}\\
		\For{$c \in RSL$}{
			$S^\prime \leftarrow S$\\
			$S^\prime$\texttt{[}$s_i$\texttt{]} $\leftarrow c$ \tcp{Move object $s_i$ to cluster $c$}
			
			\If{$f(S^\prime) < f(S)$}{
				$S \leftarrow S^\prime$\\
				$improvement \leftarrow$ \texttt{true} \\
			}
		}	
	}
	\BlankLine
	\KwRet ($S$)
	
\caption{Local Search}\label{alg:LS}
\end{algorithm}

\clearpage

\section{Experimental Setup} \label{sec:expSetup}

For our experiments we will compare the results obtained by BRKGA and SHADE over 25 datasets. Most of these datasets can be found at the \href{https://sci2s.ugr.es/keel/category.php?cat=clas}{Keel-dataset repository}\cite{triguero2017keel}, though some of them have been obtained via
\href{https://scikit-learn.org/stable/datasets/index.html}{\texttt{scikit-learn} python package} \cite{scikit-learn}. We also include 3 artificial datasets in our analysis, namely: \textit{Circles}, \textit{Moons} and \textit{Spiral}, which can be found at GitHub. Table \ref{tab:datasets} displays a summary of every dataset. 

\begin{table}[!h]
	\centering
	%\setlength{\arrayrulewidth}{1mm}
	%\setlength{\tabcolsep}{5pt}
	%\renewcommand{\arraystretch}{1.2}
	%\resizebox{\textwidth}{!}{
	\small
	\begin{tabular}{l c c c}
		\hline
		Name & No. Instances & No. Classes & No. Features \\
		\hline
		Appendicitis & 106 & 2 & 7 \\
		Boston & 506 & 229 & 13 \\
		Breast Cancer & 569 & 2 & 30 \\
		Circles & 300 & 2 & 2 \\
		Diabetes & 442 & 214 & 10 \\
		Ecoli & 336 & 8 & 7 \\
		Glass & 214 & 6 & 9 \\
		Haberman & 306 & 2 & 3 \\
		Hayesroth & 160 & 3 & 4 \\
		Ionosphere & 351 & 2 & 33 \\
		Iris & 150 & 3 & 4 \\
		Led7Digit & 500 & 10 & 7 \\
		Monk2 & 432 & 2 & 6 \\
		Moons & 300 & 2 & 2 \\
		Pima & 768 & 2 & 8 \\
		Saheart & 462 & 2 & 9 \\
		Sonar & 208 & 2 & 60 \\
		Soybean & 47 & 4 & 35 \\
		Spectfheart & 267 & 2 & 44 \\
		Spiral & 300 & 2 & 2 \\
		Tae & 151 & 3 & 5 \\
		Vehicle & 846 & 4 & 18 \\
		Vowel & 990 & 11 & 13 \\
		Wdbc & 569 & 2 & 30 \\
		Zoo & 101 & 7 & 16 \\
		\hline

	\end{tabular}%}
	\caption{Summary of datasets used for the experiments \cite{triguero2017keel}\cite{scikit-learn}}
	\label{tab:datasets}
\end{table}

\clearpage

\subsection{Constraints Generation}

\textcolor{red}{En esta seccion podria simplemente decir el numero de restricciones que genero como un numero magico sin meterme en nada de grafos, pero igual es un poco burro.}

Since we have the true labels associated with each dataset, we will use the method proposed by \cite{wagstaff2001constrained} to generate artificial constraints sets. This method consists in randomly selecting two instances of a dataset, then comparing its labels, and finally setting an ML or CL constraint depending on whether the labels are the same or different.

We will generate four different sets of constraints for each dataset. The number of constraints is given by the number of edges of the complete constraints graph that we can build with a percentage of labeled data. Each constraints set is associated with a different percentage of labeled data, namely: 5\%, 10\%, 15\% and 20\%. It is worth noting that constraints are not the complete constraint graph made up of the labeled instances, this graph only sets the number of constraints to be created, this way there is a lower probability of biasing the set of constraints so that there are classes with poor representation in it. Table \ref{tab:constraints} shows the number of constraints of each type obtained for each dataset.

\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.2}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{lcc c cc c cc c cc}
		\hline
		\multirow{2}{*}{Dataset} &
		\multicolumn{2}{c}{5\%} &&  \multicolumn{2}{c}{10\%} && \multicolumn{2}{c}{15\%} && \multicolumn{2}{c}{20\%} \\
		\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
		& ML & CL && ML & CL && ML & CL && ML & CL \\
		\hline
		Appendicitis & 13 & 2 && 39 & 16 && 71 & 49 && 164 & 67 \\
		Boston & 0 & 325 && 3 & 1272 && 13 & 2837 && 26 & 5125 \\
		Breast Cancer & 216 & 190 && 876 & 720 && 1965 & 1690 && 3487 & 2954 \\
		Circles & 50 & 55 && 208 & 227 && 502 & 488 && 853 & 917 \\
		Diabetes & 2 & 251 && 5 & 985 && 7 & 2204 && 23 & 3893 \\
		Ecoli & 30 & 106 && 163 & 398 && 357 & 918 && 609 & 1669 \\
		Glass & 11 & 44 && 52 & 179 && 139 & 389 && 259 & 644 \\
		Haberman & 76 & 44 && 304 & 161 && 634 & 401 && 1135 & 756 \\
		Hayesroth & 12 & 16 && 39 & 81 && 102 & 174 && 177 & 319 \\
		Ionosphere & 92 & 61 && 330 & 300 && 732 & 646 && 1299 & 1186 \\
		Iris & 9 & 19 && 26 & 79 && 82 & 171 && 136 & 299 \\
		Led7Digit & 25 & 275 && 126 & 1099 && 267 & 2508 && 460 & 4490 \\
		Monk2 & 101 & 130 && 473 & 473 && 979 & 1101 && 1917 & 1824 \\
		Moons & 55 & 50 && 200 & 235 && 494 & 496 && 900 & 870 \\
		Pima & 412 & 329 && 1604 & 1322 && 3595 & 3075 && 6452 & 5329 \\
		Saheart & 152 & 124 && 595 & 486 && 1292 & 1123 && 2330 & 1948 \\
		Sonar & 29 & 26 && 100 & 110 && 245 & 251 && 436 & 425 \\
		Soybean & 0 & 3 && 4 & 6 && 6 & 22 && 12 & 33 \\
		Spectfheart & 56 & 35 && 233 & 118 && 543 & 277 && 965 & 466 \\
		Spiral & 52 & 53 && 224 & 211 && 487 & 503 && 918 & 852 \\
		Tae & 8 & 20 && 40 & 80 && 82 & 171 && 151 & 314 \\
		Vehicle & 221 & 682 && 874 & 2696 && 1955 & 6046 && 3589 & 10776 \\
		Vowel & 107 & 1118 && 445 & 4406 && 1026 & 10000 && 1705 & 17798 \\
		Wdbc & 209 & 197 && 840 & 756 && 1925 & 1730 && 3472 & 2969 \\
		Zoo & 7 & 8 && 21 & 34 && 29 & 91 && 41 & 169 \\
		\hline

	\end{tabular}}
	%\end{adjustwidth}

	\caption{Number of constraints used in experiments}
	\label{tab:constraints}
\end{table}

Note that the greater the number of classes present in the dataset, the fewer ML constraints obtained with the method proposed by \cite{wagstaff2001constrained}. This is because the probability of randomly choosing two individuals from the same class decreases as the number of classes present in the dataset increases.

\clearpage

\subsection{Evaluation Method}

Since we have the true labels associated to each of the datasets, we can use them in post-processing to evaluate the results provided by each method. We will use the Adjusted Rand Index to measure the accuracy of the predictions resulting from each method we test \cite{hubert1985comparing}. The Basic Rand Index computes the degree of agreement between two partitions $C_1$ and $C_2$ of a given dataset $X$. $C_1$ and $C_2$ are viewed as collections of $N(N - 1)/2$ pairwise decisions \cite{rand1971objective}.

For each pair of instances $x_i$ and $x_j$ in $X$, $C_i$ assigns them to the same cluster or to different clusters. We take $a$ as the number of pairings where $x_i$ is in the same cluster as $x_j$ in both $C_1$ and $C_2$, and $b$ as the opposite event ($x_i$ and $x_j$ are in different clusters in $C_1$ and $C_2$). Then, the degree of similarity between $C_1$ and $C_2$ is calculated as in Equation \eqref{eq15}.

\begin{equation}
Rand(C_1, C_2) = \frac{a + b}{N(N - 1)/2}
\label{eq15}
\end{equation}

The Adjusted Rand Index is a corrected-for-chance version of the Rand Index. This correction uses the expected similarity of all comparisons between clusterings specified by a random model to set up a baseline. The Adjusted Rand Index is computed as in Equation \eqref{eq16}.

\begin{equation}
ARI(C_1, C_2) = \frac{Rand(C_1, C_2) - ExpectedIndex}{MaximumIndex - ExpectedIndex}
\label{eq16}
\end{equation}

where $MaximumIndex$ is expected to be 1 and $ExpectedIndex$ is the already mentioned expected degree of similarity with a random model. It is easy to see that $ARI(C_1, C_2) \in [-1,1]$, such that an $ARI$ value close to 1 means a high degree of agreement between $C_1$ and $C_2$, a positive value close to 0 means no agreement and a value smaller that 0 means that the $Rand(C_1, C_2)$ is less than expected when comparing with random partitions. To summarize, the higher the $ARI$, the greater the degree of similarity between $C_1$ and $C_2$. For more details on Adjusted Rand Index see \cite{hubert1985comparing}.

%% CORREGIR DESDE AQUI HASTA EL FINAL

\subsection{Calibration}

Table \ref{tab:params} shows a summary of the parameters setup used for both BRKGA+LS and SHADE+LS algorithms.

\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{>{\centering\arraybackslash}c m{5cm} cc}
			\hline
			Parameter & Meaning & BRKGA & SHADE \\
			\hline
			$p_{size}$ & Population size & 100 & 100 \\
			$Evals$ & Fitness function evaluations & 300000 & 300000 \\
			$p_e$ & Size of the elite set in population & $0.2 * p_{size}$ & $0.25 * p_{size}$ \\
			$p_m$ & Number of mutants to be introduced in the population in each generation & $0.2 * p_{size}$ & - \\
			$p_{inherit}$ & Probability that a feature is inherit from an elite parent & $50\%$ & - \\
			$K$ & Output partition number of clusters & \multicolumn{2}{m{4cm}}{Known for each dataset (Table \ref{tab:datasets}. It has the same value for both methods} \\
			\hline
			
		\end{tabular}}
		%\end{adjustwidth}
		
	\caption{Parameters setup used for BRKGA and SHADE.}
	\label{tab:params}
\end{table}

In both cases---BRKGA+LS and SHADE+LS---the population size used will be 100 individuals, and the stop criterion is given by the number of evaluations of the fitness function, which at most will be 300000.

To compare with the state of the art methods mentioned in section \ref{sec:NHres} we will use the parameters recommended by the authors, setting the maximum limit of iterations as 300 in all cases.

It should be noted that the study presented in this document aims to compare average performance, not peak performance. The parameter optimization process required for each algorithm and data set falls outside of this study.

\clearpage

\section{Experimental Results} \label{sec:results}

In this section we present the Tables \ref{tab:results5} to \ref{tab:results20}, which record the results obtained by the methods to be compared for each dataset and constraints set.

Since the methods we are comparing involve non-deterministic procedures, the results may vary from one run to another. To lessen the effect this may have on the results, we will apply each method 5 times to each dataset and set of constraints, so that the measures shown in the previously mentioned tables correspond to the average of the 5 runs.

\subsection{Comparing BRKGA + LS with SHADE + LS}

The ARI column shows the Adjusted Rand Index for each case, the Unsat column displays the percentage of violated constraints and the Time columns shows the time taken for each algorithm to deliver results, measured in seconds.


\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l ccc c ccc}
			\hline
			\multirow{2}{*}{Dataset} &
			\multicolumn{3}{c}{BRKGA + LS} &&  \multicolumn{3}{c}{SHADE + LS} \\
			\cline{2-4} \cline{6-8}
			& ARI & Unsat(\%) & Time(s) && ARI & Unsat(\%) & Time(s) \\
			\hline
			Appendicitis & 0.001 & 0.000 & 374.388 && -0.047 & 0.000 & 360.150 \\
			Boston & -0.002 & 0.000 & 1405.381 && 0.001 & 0.000 & 1425.668 \\
			Breast Cancer & 0.052 & 9.704 & 3427.396 && 0.099 & 7.685 & 3453.035 \\
			Circles & 0.020 & 1.333 & 1042.597 && 0.027 & 1.143 & 1004.287 \\
			Diabetes & 0.001 & 0.000 & 1173.404 && 0.000 & 0.633 & 1374.711 \\
			Ecoli & 0.005 & 6.912 & 1098.664 && 0.004 & 1.618 & 1045.699 \\
			Glass & -0.009 & 0.000 & 703.172 && -0.007 & 0.000 & 686.843 \\
			Haberman & 0.010 & 2.000 & 1098.043 && -0.003 & 2.000 & 1065.608 \\
			Hayesroth & -0.009 & 0.000 & 534.294 && 0.002 & 0.000 & 507.300 \\
			Ionosphere & 0.025 & 3.399 & 1821.049 && 0.044 & 3.530 & 1780.769 \\
			Iris & 0.433 & 0.000 & 494.135 && 0.670 & 0.000 & 451.290 \\
			Led7Digit & 0.006 & 4.867 & 1668.206 && 0.002 & 1.933 & 1648.570 \\
			Monk2 & 0.043 & 3.810 & 1743.874 && 0.035 & 3.637 & 1702.858 \\
			Moons & 0.081 & 1.905 & 1040.926 && 0.157 & 0.571 & 1021.028 \\
			Pima & 0.011 & 15.169 & 3803.816 && 0.014 & 12.605 & 3723.091 \\
			Saheart & 0.029 & 6.160 & 2013.580 && 0.006 & 6.739 & 1977.839 \\
			Sonar & 0.010 & 0.364 & 1076.248 && 0.001 & 0.727 & 1061.966 \\
			Soybean & 0.371 & 0.000 & 175.224 && 0.483 & 0.000 & 159.017 \\
			Spectfheart & 0.104 & 1.099 & 1395.389 && 0.084 & 0.659 & 1365.181 \\
			Spiral & 0.016 & 0.952 & 1044.697 && 0.004 & 0.953 & 1021.158 \\
			Tae & 0.004 & 0.000 & 506.495 && 0.010 & 0.000 & 476.786 \\
			Vehicle & 0.002 & 19.557 & 4008.912 && 0.003 & 10.255 & 4024.292 \\
			Vowel & 0.001 & 10.465 & 3796.154 && 0.001 & 8.000 & 3763.485 \\
			Wdbc & 0.038 & 10.246 & 3424.397 && 0.025 & 9.212 & 3400.415 \\
			Zoo & 0.016 & 0.000 & 332.475 && 0.129 & 0.000 & 318.684 \\
			\hline

		\end{tabular}}
		%\end{adjustwidth}

	\caption{Experimental results obtained with 5\% of labeled data}
	\label{tab:results5}
\end{table}

Table \ref{tab:results5} shows the results for the constraints set obtained with 5\% of the labeled data. We can note that SHADE+LS represents a consistent improvement over BRKGA + LS in both Unsat and Time, while in ARI both methods seems similar. This is because, operating with a small set of constraints, the penalty applied to the fitness function of both methods has little presence, and since they are population-based and heurist-guided methods, they have similar exploration and exploitation capabilities.

\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l ccc c ccc}
			\hline
			\multirow{2}{*}{Dataset} &
			\multicolumn{3}{c}{BRKGA + LS} &&  \multicolumn{3}{c}{SHADE + LS} \\
			\cline{2-4} \cline{6-8}
			& ARI & Unsat(\%) & Time(s) && ARI & Unsat(\%) & Time(s) \\
			\hline
			Appendicitis & 0.022 & 5.454 & 383.642 && 0.086 & 5.091 & 366.491 \\
			Boston & -0.000 & 0.173 & 1502.452 && -0.001 & 0.235 & 1570.547 \\
			Breast\_Cancer & 0.395 & 17.444 & 3651.263 && 0.159 & 22.256 & 3602.665 \\
			Circles & 0.166 & 12.598 & 1077.238 && 0.063 & 14.207 & 1060.787 \\
			Diabetes & 0.000 & 0.364 & 1277.982 && 0.003 & 0.465 & 1284.486 \\
			Ecoli & 0.009 & 20.321 & 1169.798 && 0.020 & 12.763 & 1112.981 \\
			Glass & 0.016 & 8.658 & 735.507 && 0.013 & 4.848 & 694.962 \\
			Haberman & 0.047 & 15.699 & 1155.985 && 0.084 & 15.441 & 1146.023 \\
			Hayesroth & 0.027 & 1.500 & 550.117 && 0.025 & 2.000 & 529.424 \\
			Ionosphere & 0.125 & 17.587 & 1894.390 && 0.332 & 12.603 & 1872.308 \\
			Iris & 0.238 & 0.190 & 505.112 && 0.342 & 0.000 & 483.584 \\
			Led7Digit & 0.009 & 11.347 & 1786.943 && 0.004 & 8.866 & 1784.708 \\
			Monk2 & 0.421 & 14.926 & 1848.543 && 0.270 & 16.385 & 1821.208 \\
			Moons & 0.277 & 11.816 & 1098.089 && 0.272 & 11.356 & 1075.089 \\
			Pima & 0.236 & 25.195 & 4072.087 && 0.275 & 22.468 & 3990.383 \\
			Saheart & 0.147 & 20.555 & 2135.624 && 0.265 & 16.725 & 2095.554 \\
			Sonar & 0.124 & 7.619 & 1106.039 && 0.116 & 8.952 & 1081.994 \\
			Soybean & 0.496 & 0.000 & 176.333 && 0.515 & 0.000 & 151.163 \\
			Spectfheart & 0.334 & 9.573 & 1451.235 && 0.343 & 8.604 & 1405.243 \\
			Spiral & 0.058 & 15.402 & 1098.622 && 0.114 & 13.793 & 1099.747 \\
			Tae & 0.018 & 0.833 & 520.855 && 0.024 & 1.333 & 508.988 \\
			Vehicle & 0.005 & 28.515 & 4366.104 && 0.006 & 22.151 & 4352.663 \\
			Vowel & 0.003 & 13.330 & 4245.672 && 0.002 & 11.523 & 4290.154 \\
			Wdbc & 0.478 & 16.316 & 3596.517 && 0.357 & 17.093 & 3528.029 \\
			Zoo & 0.105 & 1.091 & 336.345 && 0.098 & 2.182 & 317.674 \\
			\hline
			
		\end{tabular}}
		%\end{adjustwidth}
		
	\caption{Experimental results obtained with 10\% of labeled data}
	\label{tab:results10}
\end{table}

The Table \ref{tab:results10} presents the results obtained for the constraints set based on 10\% labeled data. On the results for Unsat and Time, we can draw similar conclusions to those obtained with Table \ref{tab:results5}, that is, the advantage remains for SHADE + LS, even with the general increase of these values derived from a larger set of constraints. It is at ARI where we begin to see the difference between the two methods.
	
\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l ccc c ccc}
			\hline
			\multirow{2}{*}{Dataset} &
			\multicolumn{3}{c}{BRKGA + LS} &&  \multicolumn{3}{c}{SHADE + LS} \\
			\cline{2-4} \cline{6-8}
			& ARI & Unsat(\%) & Time(s) && ARI & Unsat(\%) & Time(s) \\
			\hline
			Appendicitis & 0.296 & 6.167 & 381.625 && 0.332 & 5.833 & 362.106 \\
			Boston & 0.000 & 0.526 & 1648.008 && 0.001 & 0.547 & 1688.985 \\
			Breast\_Cancer & 0.772 & 10.172 & 3854.911 && 0.911 & 3.310 & 3766.402 \\
			Circles & 0.828 & 6.101 & 1136.484 && 0.982 & 0.404 & 1108.928 \\
			Diabetes & 0.004 & 0.317 & 1364.941 && -0.000 & 0.371 & 1424.047 \\
			Ecoli & 0.015 & 24.314 & 1200.463 && 0.025 & 19.090 & 1127.568 \\
			Glass & 0.017 & 17.500 & 745.987 && 0.031 & 11.743 & 722.009 \\
			Haberman & 0.866 & 4.677 & 1209.471 && 0.796 & 5.314 & 1147.479 \\
			Hayesroth & 0.107 & 7.536 & 545.861 && 0.099 & 8.261 & 520.702 \\
			Ionosphere & 0.803 & 7.722 & 1986.543 && 0.591 & 11.974 & 1944.085 \\
			Iris & 0.412 & 3.873 & 506.548 && 0.260 & 6.087 & 486.723 \\
			Led7Digit & 0.004 & 12.836 & 1902.272 && 0.007 & 10.919 & 1897.511 \\
			Monk2 & 0.751 & 10.827 & 1949.305 && 0.945 & 2.067 & 1920.361 \\
			Moons & 0.958 & 1.334 & 1133.858 && 0.995 & 0.020 & 1113.687 \\
			Pima & 0.481 & 22.051 & 4451.193 && 0.762 & 9.928 & 4402.693 \\
			Saheart & 0.749 & 10.940 & 2281.992 && 0.945 & 1.839 & 2225.210 \\
			Sonar & 0.800 & 5.040 & 1124.700 && 0.890 & 2.177 & 1069.862 \\
			Soybean & 0.428 & 0.000 & 170.674 && 0.422 & 0.000 & 155.948 \\
			Spectfheart & 0.924 & 2.854 & 1561.040 && 0.778 & 5.732 & 1485.260 \\
			Spiral & 0.847 & 5.677 & 1133.962 && 0.949 & 1.495 & 1102.920 \\
			Tae & 0.119 & 7.115 & 522.088 && 0.066 & 8.380 & 513.289 \\
			Vehicle & 0.089 & 28.044 & 4806.559 && 0.018 & 26.856 & 4749.812 \\
			Vowel & 0.002 & 14.486 & 4807.165 && 0.001 & 13.341 & 4867.849 \\
			Wdbc & 0.805 & 8.985 & 3829.407 && 0.930 & 2.801 & 3750.711 \\
			Zoo & 0.157 & 3.500 & 333.984 && 0.128 & 2.834 & 315.131 \\
			\hline
			
		\end{tabular}}
		%\end{adjustwidth}
		
	\caption{Experimental results obtained with 15\% of labeled data}
	\label{tab:results15}
\end{table}

Table \ref{tab:results15} presents the results obtained with 15\% of labeled data. While conclusions for Unsat and Time remain unchanged, we see a clear difference as far as ARI is concerned. We observe that, for constraints sets obtained with 15\% labeled data, both methods already report ARI results above 0.7, which in an ARI ranking is considered to be a good result. We can even observe that the SHADE + LS method provides an ARI higher than 0.9 for 7 datasets, compared to BRKGA + LS that only obtains similar results in 2 datasets.
	
\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l ccc c ccc}
			\hline
			\multirow{2}{*}{Dataset} &
			\multicolumn{3}{c}{BRKGA + LS} &&  \multicolumn{3}{c}{SHADE + LS} \\
			\cline{2-4} \cline{6-8}
			& ARI & Unsat(\%) & Time(s) && ARI & Unsat(\%) & Time(s) \\
			\hline
			Appendicitis & 1.000 & 0.000 & 389.905 && 1.000 & 0.000 & 371.579 \\
			Boston & 0.001 & 0.636 & 1820.313 && 0.001 & 0.660 & 1878.863 \\
			Breast\_Cancer & 0.807 & 9.086 & 4061.162 && 0.977 & 0.891 & 3984.370 \\
			Circles & 0.935 & 2.814 & 1180.268 && 1.000 & 0.000 & 1170.452 \\
			Diabetes & 0.003 & 0.695 & 1480.035 && 0.001 & 0.710 & 1589.821 \\
			Ecoli & 0.028 & 25.716 & 1267.419 && 0.047 & 21.466 & 1277.903 \\
			Glass & 0.038 & 23.610 & 766.861 && 0.082 & 16.412 & 796.162 \\
			Haberman & 0.943 & 2.073 & 1266.131 && 0.997 & 0.138 & 1235.441 \\
			Hayesroth & 0.548 & 6.331 & 554.339 && 0.531 & 7.056 & 540.585 \\
			Ionosphere & 0.902 & 4.314 & 2051.251 && 0.993 & 0.258 & 2019.558 \\
			Iris & 0.691 & 5.241 & 513.675 && 0.553 & 6.528 & 488.262 \\
			Led7Digit & 0.007 & 13.653 & 2053.264 && 0.005 & 12.170 & 2066.223 \\
			Monk2 & 0.855 & 6.025 & 2055.425 && 0.991 & 0.379 & 2013.468 \\
			Moons & 0.937 & 2.689 & 1177.848 && 0.995 & 0.158 & 1157.222 \\
			Pima & 0.753 & 11.544 & 4855.446 && 0.899 & 4.419 & 4820.684 \\
			Saheart & 0.881 & 5.250 & 2402.513 && 0.981 & 0.706 & 2353.931 \\
			Sonar & 0.992 & 0.186 & 1138.769 && 1.000 & 0.000 & 1127.020 \\
			Soybean & 0.416 & 0.000 & 171.281 && 0.563 & 0.000 & 153.406 \\
			Spectfheart & 0.953 & 1.677 & 1602.229 && 1.000 & 0.000 & 1526.925 \\
			Spiral & 0.921 & 3.604 & 1178.580 && 0.998 & 0.091 & 1155.194 \\
			Tae & 0.351 & 9.807 & 527.292 && 0.392 & 9.419 & 508.610 \\
			Vehicle & 0.144 & 28.781 & 5189.066 && 0.044 & 29.008 & 5284.288 \\
			Vowel & 0.002 & 14.682 & 5450.331 && 0.003 & 13.766 & 5514.052 \\
			Wdbc & 0.823 & 8.281 & 4019.683 && 0.972 & 1.068 & 3983.564 \\
			Zoo & 0.176 & 5.429 & 335.715 && 0.153 & 4.191 & 320.186 \\
			\hline
			
		\end{tabular}}
		%\end{adjustwidth}
		
	\caption{Experimental results obtained with 20\% of labeled data}
	\label{tab:results20}
\end{table}

It is in Table \ref{tab:results20}, which gathers results obtained with constraints sets generated from 20\% of labeled data for each dataset, that we observe the major differences between the two methods. We found that SHADE + LS is capable of producing clusters identical to the original ones for 4 datasets, against the single dataset for which BRKGA + LS obtains a similar result. Let's recall that an ARI of 1 represents a total matching between the two partitions given as arguments for ARI's calculation. This improvement in ARI is clearly reflected in the percentage of violated constraints--Unsat column---, value for which, once again, SHADE + LS gets the advantage. Conclusions on execution time remain the same.

\subsection{Comparing SHADE + LS with the state-of-the-art}

\clearpage

\section{Results Analysis} \label{sec:analisis}

With the results obtained by both methods for a total of 100 different datasets---the 25 datasets in combination with the 4 constraints sets for each one of them--we can perform an empirical analysis of them. This way we can statistically determine which of the two methods offers the best results.

In order to carry out this task we will make use the widely known frequentist non-parametric Wilcoxon's signed rank test, as well as the lesser-known Bayesian sign test.

From now on, and as far as this section is concerned, we will refer to the results obtained by BRKGA - LS as sample $A$, and to the results obtained with SHADE + LS as sample $B$.

\subsection{Applying Wilcoxon's signed rank test to our results}

We will apply the Wilcoxon's signed rank test for each of the measures obtained, namely: ARI, Unsat and Time. We set as null hypothesis $H_0 = $ the distribution of $A - B$ is symmetric about $\mu$. However, given that in the case of ARI it is a problem of maximisation, and in the case of Time and Unsat it is a problem of minimisation, we will consider a different alternative hypothesis for each case. In the case of maximization we propose $H_{max} =$ the distribution of $A - B$ is shifted to the left with respect to $\mu$. On the other hand, in the case of minimization we propose $H_{min} =$ the distribution of $A - B$ is shifted to the right with respect to $\mu$. In this way we assume that SHADE + LS is better than BRKGA + LS and we try to prove it.

In this case we set $\mu = 0$, and work with a confidence level of 95\%, which means that the significance level is 0.05 ($\alpha = 0.05$). Table \ref{tab:freqTest} shows the statistical values obtained using Wilcoxon's test for each comparison.
	
\begin{table}[!htp]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	\begin{tabular}{ccccc}
		\hline
		Measure & $R^{+}$ & $R^{-}$ & P-value & Alternative hypothesis  \\
		\hline
		ARI & 3088 & 1665 & 0.005254 & $H_{max}$ \\
		Time & 4008 & 1042 & 0.0000001723 & $H_{min}$ \\  
		Unsat & 3082.5 & 745.5 & 0.0000003838 & $H_{min}$ \\ 
		\hline 
	\end{tabular}
	\caption{Results obtained by the Wilcoxon's signed rank test for the comparison SHADE vs BRKGA}
	\label{tab:freqTest}
\end{table}

For all three considered measures, $P-value < \alpha$, therefore in all cases we can reject $H_0$ and find that there are significant differences between the two methods, giving the adavantage to SHADE + LS with basis to the proposed alternative hypothesis. In conclusion, we can safely admit that Wilcoxon's signed rank test provides empirical evidence in favour of SHADE + LS.

\subsection{Applying Bayesian sign test to our results}

Te Bayesian sign test is the Bayesian version of the frequentist non-parametric sign test. It allows us to conclude whether there are significant differences between two sets of results.\cite{carrasco2017rnpbst}

It is based on obtaining the $\rho$ parameter of a statistical distribution according to the difference between the results. To get the distribution of $\rho$ we count the number of times that $A - B<0$, the number of times where there are no significant differences, and the number of times that $A - B > 0$. In order to identify cases where there are no significant differences we define the region of practical equivalence (rope) $[r_{min}, r_{max}]$, so that $P(A = B) = P(\rho \in rope)$. Based on these results we calculate the weights of a Dirichlet distribution and sample it to get a set of triplets in the form of $[(\rho < r_{min}), (\rho \in rope), (\rho > r_{max})]$.

We can produce a representation of the triplet set in the form of a heatmap where each triplet constitutes one point. With this in mind, we will associate each of the triplet values with each of the three vertices of an equilateral triangle. In order to find out where a certain triplet will be placed within the triangle, we will take each of its three values and draw a parallel line to the opposing side of the corresponding vertex; the separation between a triangle side and its parallel line will be proportional to the associated triplet value, so that the higher the value, the closer the line will be to the vertex. The location where the three lines intersect is where we draw a point. Since the values of every triplet describe a probability distribution and therefore they must add up to one, we can be sure that all triplets will lie in some point within the triangle. The color indicates the density of points in a given region, with yellow representing a high density and red a low density.

\begin{figure}[ht!]
	\centering
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=1\linewidth]{Figures/BayesSignoARI.pdf}
		\caption{ARI}
		\label{fig:bayesARI}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=1\linewidth]{Figures/BayesSignoTime.pdf}
		\caption{Time}
		\label{fig:bayesTime}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=1\linewidth]{Figures/BayesSignoUnsat.pdf}
		\caption{Unsat}
		\label{fig:bayesUnsat}
	\end{subfigure}
	\caption{Heat diagrams for each measure}
\end{figure}

Figure \ref{fig:bayesARI} shows the heatmap associated with the ARI measurement. The diagram suggest that the Bayesian sign test assigns a high probability to $A - B < 0$, since most triplets are represented in the lower left third of the triangle. It also indicates that in some cases there are no significant differences between $A$ and $B$, since we also find triplets in the rope area. Bearing this in mind, and considering that ARI is a measure to maximize, we can say that, for cases where there are significant differences, SHADE + LS performs better than BRKGA + LS.

On the other hand, Figures \ref{fig:bayesTime} and \ref{fig:bayesUnsat} are very similar, they show the heatmaps obtained for Time and Unsat measurements. In both, all triplets are represented in the lower right third of the triangle, indicating that the Bayesian sign test assigns a high probability to $A - B > 0$. With this, and bearing in mind that both measures must be minimized, we conclude that SHADE + LS represents a solid improvement over BRKGA + LS as far a this measures are concerned.


\section{Conclusions} \label{sec:conclusiones}

In this paper we proposed an adaptation of SHADE to solve the constrained clustering problem. We took concepts such as the random-key and the encoder/decoder from BRKGA to transform constrained clustering into a real-domain optimization problem. We also proposed a new cost function that has proven to be more adaptable and less restrictive.

The SHADE algorithm is one of the most successful methods to approximately solve difficult problems, and constrained clustering does not seem to be an exception. We obtained results on 25 different data sets and 4 different constraints sets for each, applying all the resolution methods named in this paper. Thanks to statistical tests, like Wilcoxon's signed rank test, Bayesian signed rank test and Friedman's test, we were able to objectively prove that the SHADE + LS approach is significantly better than previous proposals. This last statement also applies to the comparison with some of the state-of-the-art methods.



\section{Acknowledgements}

\clearpage

\section*{References}

\bibliography{mybibfile}

\end{document}

\begin{figure}[bth]
	\myfloatalign
	%\subfloat[ARI]
	{\includegraphics[width=.45\linewidth]{Figures/BayesSignoARI.pdf}} \quad
	%\subfloat[Time]
	{\includegraphics[width=.45\linewidth]{Figures/BayesSignoTime.pdf}} \quad
	%\subfloat[Unsat]
	{\includegraphics[width=.45\linewidth]{Figures/BayesSignoUnsat.pdf}} \quad
	\caption{Heat diagrams for each measure obtained.}
	\label{img:heatDiag}
\end{figure}

\begin{table}[!h]
	\centering
	\setlength{\tabcolsep}{7pt}
	\renewcommand{\arraystretch}{1.4}
	%\begin{adjustwidth}{-1in}{-1in}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c >{\centering\arraybackslash}c m{5cm} c}
			\hline
			Algorithm & Parameter & Meaning & Value \\
			\hline
			All & $MaxIter$ & Iterations limit for the algorithm & 300 \\
			\multirow{2}{*}{COPKM}
			& $Tolerance$ & Used to determine the lower threshold limit of centroid movement & $1 \times 10^{-4}$ \\
			& $InitMethod$ & Method for centroid initialization & Random \\
			\multirow{5}{*}{CEKM}
			& $\alpha$ & exponente que controla la penalización
			por asignar instancias a conjuntos con alta cardinalidad &
			\hline \\
			\hline
			
		\end{tabular}}		
		\caption{Parameters setup used for the state-of-the-art methods.}
		\label{tab:paramsSOTA}
	\end{table}