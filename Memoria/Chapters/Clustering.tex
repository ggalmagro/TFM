
\chapter{Short Introduction to Clustering}\label{ch:IntroClustering}

Firstly, it will be necessary to provide an introduction, albeit a brief one, to clustering and its applications, in order to ease the understanding of the following chapters. For this purpose we will follow the study carried out in \cite{Everitt:2009:CA:1538772}.

\section{Reasons for Classifying}

Classification can be understood as a way of simplifying the information contained in large datasets in a way that is easily understood by humans. In this way, the processes of extracting useful information and applying it are simplified. Thus, if we are able to meaningfully divide a large dataset into subsets or groups, we can extract information common to elements of one of these subsets and provide a precise description that encompasses them.

The need to analyze information in this way grows with the emergence and availability of large datasets in science. The analysis of this type of information by classification, or clustering, is known today as \textit{Data Science}. In the 21st century a particular interest in data science arises from the appearance of the \textit{World Wide Web}, known as the Internet, where the aim has become to extract relevant information from the Web pages that make up this vast network.

It is important to note that, in most cases, there is no single classification criterion for the same dataset, but rather a wide variety of data. In the case of individuals, they could be classified, for example, on the basis of their income, or according to the amount of calories they consume over a defined period of time. Thus, different classification criteria do not necessarily result in the same division into groups of the set to be classified; in that way, different criteria will serve different purposes.

\section{Numerical Methods of Classification}

Numerical methods for clustering arise in branches of natural sciences, such as biology or zoology, in an attempt to eliminate the subjectivity implicit in the classification process triggered by the discovery of a new species. The aim is to provide a non-subjective and stable method for classifying and grouping elements.

These methods adopt different names that depend on the field in which they are applied: numerical taxonomy in biology, $Q$ analysis in psychology, or unsupervised pattern recognition in \acf{AI}. However, today, cluster analysis, or just clustering, are the most widely accepted and extended terms for tasks that involve the discovery of subgroups within a set of elements.

In most clustering applications, the goal is to obtain a data partition, in which each instance or object belongs to a single cluster, and the union of all of them contains all the individual objects. That said, it should be noted that, in some circumstances, solutions where there is overlap between clusters are acceptable, as well as there may not be an acceptable data partition.

In the literature, clustering methods are often divided into two subsets: partitional clustering and hierarchical clustering. In hierarchical clustering the result is not a partition of the data with a certain number of clusters---as in partitional clustering---, but instead a dendrogram in which there are partitions that include from the whole dataset to particular individuals \cite{Everitt:2009:CA:1538772}. A representative example of partitional clustering is the widely studied and well-known algorithm K-means \cite{wu2009top}, while for hierarchical clustering the CURE method should be highlighted \cite{guha1998cure}. In this work we will focus on partitional clustering.

The most common way to represent the information on which clustering is applied is an array $X$ of dimension $n\times u$, in which each row corresponds to an instance or object to be processed, and each column corresponds to one of the variables that characterize those instances or objects. The commonly accepted term to refer to each row is \textit{features vector}, and the features vector set is referred as \textit{dataset}.


$$ X = \left( \begin{array}{cccc}

x_{1,1} & x_{1,2} & \cdots & x_{1,u} \\

x_{2,1} & \cdots & \cdots & \cdots  \\

\vdots & \vdots & \vdots & \vdots \\

x_{n,1} & \cdots & \cdots & x_{n,u} \\

\end{array} \right) $$

The entry $x_{i,j}$ in $X$ corresponds to the value of the variable $j$th in the instance $i$. We will refer to the $i$th instance in $X$ as $x_i = (x_{[i,1]}, \cdots, x_{[i,u]})$, which is a feature vector.

We can define partitional clustering as the task of grouping the instances of the dataset $X$ into $k$ clusters, so that new information can be extracted from them. A typical clustering algorithm assigns a class label $l_i$ to each instance $x_i \in X$. As a result, we obtain the set of labels $L = \{l_1, \cdots, l_n\}$, with $l_i \in \{1, \cdots, k\}$, that effectively splits $X$ into $k$ non-overlapping clusters $\{c_i, \cdots, c_k\}$ to form a partition called $C$. The criterion used to assign an instance to a given cluster is the similarity to the rest of elements in that cluster, and the dissimilarity to the rest of instances of the dataset, which can be obtained with some kind of distance measurement $d(\cdot,\cdot)$ \cite{jain1999data}.

Variables in $X$ can be a mixture of attributes in a continuous, discrete, or categorical domain. In addition, it is possible that, in real problems, some entries may not be available. This mix of variable types and missing values can complicate the clustering task. However, there are methods to deal with these cases, such as inference of missing values or domain transformations.

In some applications, matrix rows may contain repeated measurements of the same variable, although under different conditions, or at different times, even at different spatial locations. An example of this can be the height measurements of a group of children in the same month over different years. This type of data has a structure that, again, can complicate the task of clustering.

Some clustering methods involve making transformations on the $X$ matrix to convert it into a $n \times n$ matrix in which measurements extracted from the $X$ matrix are stored that relate one instance to all the others, such as similarity, distance or dissimilarity.

Clustering is, put simply, the discovery of groups in data, and should never be confused with methods of discrimination or assignment, known in \acs{AI} as Supervised Learning, as we have already mentioned in Section \ref{sec:TypesOfML}. 

Once the general structure of clustering methods has been defined, it would be justified to ask, what is a cluster? Section \ref{sec:WhatIsCluster} will try to answer this question.

\subsection{What is a Cluster?} \label{sec:WhatIsCluster}

So far, the terms cluster, group and class have been used in a completely intuitive way, with no need for any formal definition, a further proof of the innate nature of these concepts in the human being. In fact, giving a formal definition of cluster is a task, not only complicated, but in many occasions not very useful. For example, in \cite{lance1967general} a cluster definition completely dependent on the user's interpretation is proposed. As far as this definition is concerned, a cluster is what the user understands as a cluster without having proposed a formal definition of it.

\begin{quotation}{\slshape
		The value judgment of the user is the ultimate criterion for evaluating the meaning of these terms [cluster]. If using them produces an answer of value, no more need be asked of them.}
	\begin{flushright}
		\textbf{R.E. Bonner, On Some Clustering Techniques, 1964} 
	\end{flushright}
\end{quotation}

Although the definition above is accurate in a wide range of situations, a somewhat more analytical definition from the mathematical point of view, defining a cluster in terms of internal cohesion---homogeneity---, and external isolation---separation is proposed in \cite{cormack1971review} and \cite{gordon:1999}. Figure \ref{fig:ClustersProperties} informally illustrates the properties described above, so that any observer will find the clusters present in it apparent without the need for a formal cluster definition. This may explain why achieving a mathematically accurate definition of homogeneity and separation may become unnecessary.

\begin{figure}[bth]
	\myfloatalign
	{\includegraphics[width=.3\linewidth]{gfx/Clustering/TwoBasicsClusters}}
	{\includegraphics[width=.3\linewidth]{gfx/Clustering/MoonsBasics}}
	{\includegraphics[width=.3\linewidth]{gfx/Clustering/ThreeBasicClusters}}
	\caption[Clusters with internal cohesion and/or external isolation]{Clusters with internal cohesion and/or external isolation}\label{fig:ClustersProperties}
\end{figure}

It is not entirely clear how people recognize different clusters when they are represented on a plane, but one of the variables that certainly influences is the distribution of relative distances between objects or points.

On the other hand, as mentioned earlier in this section, it may be the case that there is no justified partition in a dataset. Figure \ref{fig:uniform_cloud} shows a dataset for which most observers would conclude that there are no distinct groups, just a cloud of uniformly distributed points. Ideally, one would expect a clustering method applied to this same dataset to reach the same conclusion.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.2]{gfx/Clustering/rand.png} 
	\caption{Cloud of uniformly distributed points.}\label{fig:uniform_cloud}
\end{figure}

However, most unsupervised learning methods will result in uniform partitioning as shown in Figure \ref{fig:uniform_partition}. The number of partitions found will depend on the method applied, although uniform partitioning will be achieved in any case.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.2]{gfx/Clustering/randClasif.png} 
	\caption{Classified cloud of uniformly distributed points.}\label{fig:uniform_partition}
\end{figure}

The process of dividing a homogeneous distribution of data into different groups is known as dissection, and such a process may be useful in certain circumstances. However, since in most cases of actual application of cluster methods, the structure of the data is not known a priori, there is risk of interpreting all solutions in terms of the existence of subgroups, which would lead to the imposition of a fictitious structure on data in which there is no structure present.

\section{Applications of Clustering}

As it has been already mentioned, the general problem clustering attempts to solve is present in many disciplines: biology, botany, medicine, psychology, geography, marketing, image processing, psychiatry, archaeology, etc. This section presents some of the clustering applications related to these and others knowledge fields.

\subsection{Marketing}

Dividing customers into homogeneous groups is one of the most common marketing tasks. A marketing manager might wonder how to group potential customers according to the potential benefits of the product he or she is trying to introduce to the market. On the other hand, a marketing analyst might be interested in grouping companies according to their financial characteristics, in order to be able to analyze them and predict their market strategies.

An example of the application of clustering in this field was published in \cite{green1967cluster}. With a large number of cities available for analysis, they had to restrict the places in which to carry out their studies for economic reasons. To this end they relied on cluster analysis, classifying cities into small groups based on 14 city features, including size and average income \textit{per capita}. Since cities in the same group were expected to be very similar, they chose one city from each of them to conduct their studies.

Another application of cluster analysis in marketing was described in \cite{chakrapani2004statistics}. In this case, a car manufacturer believes that buying a sports car is not a decision based solely on economic capabilities or age, but is a lifestyle decision made by those who decide to buy a sports car, as opposed to those who do not. Consequently, the manufacturer decides to carry out a study, using cluster analysis, which allows it to identify all features related to the people who would buy a sports car, in order to focus its marketing campaigns specifically on this sector.

\subsection{Astronomy}

Given a set of astronomical data, researchers want to know, for example, how many classes of stars are present in them, based on some statistical criteria. The most frequently asked questions in this area are: how many statistically different objects are present in the data and to which class should each object be assigned? Do previously unknown classes of objects appear? Cluster analysis can be applied to answer these questions, helping to detect statistically anomalous objects, as well as to guide the process of classifying them. Examples include the discovery of high redshift quasars, type 2 quasars (highly luminous, active galactic nuclei often obscured by dust and gas), and brown dwarfs.

In \cite{faundez1996classification} a specific example of the above can be found. Clustering techniques are applied to data on the chemical composition of 192 planetary nebulae. Six different groups were identified, which were similar in many respects to a previous classification of such objects, but also showed interesting differences that researchers had so far missed.

Clustering based on normal distributions is applied to a set of 2370 stars in \cite{celeux1992classification}. They were described by their relative speed to the galactic nucleus and galactic rotation. Using a model of three clusters, they found one cluster of large size and small volume, and two of small size and large volume.

\subsection{Psychiatry}

Mind disorders are often more difficult to diagnose than body pathologies, which is why there has been a growing interest in cluster analysis techniques to refine, or even redefine, diagnostic techniques for this type of disease in the field of psychiatry. Much of this work involves depressed patients, cases in which the interest lies in distinguishing between two types of depression, endogenous (congenital) and neurotic.

In \cite{pledger2008using} clustering techniques are applied to 200 patients based on their responses to a questionnaire about depression, along with information about their sex, age, mental state, and illness. This is a clear example of different types of variables included in the same dataset. One of the groups obtained as a result of this study was identified as a marker of endogenous depression.

Cluster analysis has also been used to find a classification of individuals who attempted suicide, which could lay the groundwork for further studies on the causes and treatments of the problem. In \cite{paykel1978classification} a study on 236 cases of failed suicides recorded by the emergency service of a city in the United States of America is presented. From the set of available variables, 14 were selected as particularly relevant for classification and, therefore, were used in the analysis. Among them were: age, number of suicide attempts, severity of depression and degree of hostility, in addition to a series of demographic features. Clustering methods were applied to the resulting dataset; the most significant result obtained corresponds to a division into three well-defined clusters.

\subsection{Meteorology and Climatology}

Huge amounts of global weather data are collected daily. Exploring these data using clustering techniques can bring new approaches to study climatology and environmental phenomena.

In \cite{littmann2000empirical} clustering techniques are applied to data collected on daily changes in surface pressure in the Mediterranean basin, 20 groups explaining the variance of rainfall in the central Mediterranean regions were found. Another example can be found in \cite{liu2005mining}, in which the Fuzzy K-means algorithm is applied to spatial-temporal data of the USA southern-central regions climatology. 

\subsection{Archaeology}

Archaeology is another discipline in which clustering is useful. The classification of different objects found in deposits can help to discover their use, the periods to which they belong, as well as the population that used them. Similarly, studying fossilized materials can help reveal how prehistoric societies lived. 

An early example of the application of clustering to archaeological objects is given in \cite{hodson1966some}, where clustering techniques are applied to a group of brooches dating from the Iron Age, finding a classification for them of proven archaeological relevance. Another example found in \cite{hodson1971numerical} is the application of the K-means algorithm to construct a taxonomy of hand axes found in the British Isles. The variables taken into account to describe each axe include length, width and pointedness. Clustering resulted in two groups of axes, one made up of small and thin axes and the other made up of large and thick axes.

Regarding fossilized materials, in \cite{sutton1995cluster} a study of 155 coprolites found at Antelope House, a prehistoric site at Chelly Canyon in Arizona is conducted. The study resulted in a diet-based interpretation of the differences between coprolites.

\subsection{Bioinformatics and Genetics}

Recent times are witnessing a tremendous boom in interest in Bioinformatics, complemented by molecular biology, computer science, mathematics and statistics. Such an increase has been accelerated by the ever-growing database of genomic and protein data, which are themselves the result of a major advance in DNA sequencing techniques, gene expression measurements, and compression of macromolecular structures. Statistics have been relevant in the study of gene expression. The genes contained in the DNA of each cell provide the necessary templates for the generation of the proteins involved in most of the structural and biomechanical processes that take place in each of us. However, although most cells in humans contain all the genetic components that make up the human genome, genes are expressed selectively in each cell depending on cell type, tissue and general conditions inside and outside the cell. Molecular biology has shown that most of the processes in a cell's life are regulated by factors that affect the expression of its genes.

As we have seen, one of the most active fields of research today is the study of the processes that regulate gene expression. In order to store the information related to this area of study, microarrays arise \cite{cortese2000array}. From the data analysis point of view, one of the relevant features in this type of information is that the number of attributes of each instance ($u$) far exceeds the number of available instances ($n$); datasets featuring this property are called \textit{high dimensionality datasets}.

Most classic statistical methods cannot be applied to this type of dataset without being substantially modified. However, cluster analysis accepts such datasets and can be used to identify groups of genes with similar expression patterns. This allows us to answer questions such as why a gene is affected by a certain disease, or which genes are responsible for hereditary genetic diseases.

An example of application is found in \cite{selinski2008cluster}, who used clustering on single-nucleotide polymorphisms to detect differences between diseases at the genetic level.

\section{Recent Applications}

Although the examples of practical clustering applications presented so far are illustrative, it should be noted that clustering techniques are applied in a multitude of modern problems. Since it is not humanly possible to cover all the practical uses of clustering, a brief list of some of them is presented here: computer vision-guided traffic management \cite{kumaran2019computer}, near real-time detection of
spatial clusters from large position data streams generated
by moving objects \cite{junior2019dg2cep}, wireless sensor networks management \cite{wan2019similarity}, internet of things \cite{aranzazu2019anchor}, detecting taxi movements \cite{ibrahim2019detecting}, \textit{big data} distributed multi-target regression \cite{corizzo2019dencast},  cartographic labeling \cite{araujo2019improving} and image segmentation in various fields, ranging from general applications \cite{wang2018non} to very specific ones such as in medicine \cite{verma2016improved, aparajeeta2016modified}. 

It seems clear that clustering is an extremely powerful tool, no only for the development of any branch of science, but also because it can be applied to  many real-world problems and directly help people.

\section{Summary}

Clustering consists of exploring datasets. Its goal is to determine whether or not they can be summarized in a meaningful way, in terms of a relatively small number of groups or clusters of objects or individuals which resemble each other and differ from those found in other clusters.

Many branches of science have successfully made use of clustering techniques to advance in their respective fields and to process large amounts of data, whose analysis would be unthinkable to tackle with traditional techniques.

%Para referir un acronimo completo \acf
%Para referir un acronimo solo con las siglas \acs




















